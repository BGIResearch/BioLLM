# the param for the llm model, to init the foundation model.
model_used = "scbert"
model_file = '../../case/models/scbert/panglao_pretrain.pth'
vocab_file = '../../case/models/scbert/gene_vocab.json'
g2v_file = '../../case/models/scbert/gene2vec_16906.npy'
# model params, always not change
bin_num = 7
embsize = 200
max_seq_len = 16907
batch_size = 8
use_g2v = true
GRADIENT_ACCUMULATION = 60
UNASSIGN_THRES = 0.0
# task params, user setting for the task
device = 'cuda:1'
distributed = true
finetune = true
# the input data info, for the preocess step
#input_file = '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/bio_model/biollm/case/data/annotation/human/organs/zheng68k/train.h5ad'
input_file = '/home/share/huadjyin/home/s_huluni/yanbang/data/train.h5ad'
output_dir = '../../case/result/anno/scbert/zheng68k_yb/'
do_preprocess = true
label_key = 'celltype'
# trainning params
lr = 0.001
epochs=20
early_stop=15
# wandb setting
weight_bias_track = true
project_name = 'biollm_annotation'
exp_name = 'scbert_zheng68k'
