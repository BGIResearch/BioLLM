{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/share/huadjyin/home/s_huluni/project/bio_model/biollm\")\n",
    "\n",
    "import os\n",
    "working_dir = \"/home/share/huadjyin/home/s_huluni/project/bio_model/biollm/biollm\"\n",
    "os.chdir(working_dir)\n",
    "\n",
    "from biollm.base.load_geneformer import LoadGeneformer\n",
    "cfs_file = \"config/geneformer_grn.toml\"\n",
    "model = LoadGeneformer(cfs_file=cfs_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:21:32.703812100Z",
     "start_time": "2024-04-03T08:21:09.616412Z"
    }
   },
   "id": "dac6193865048613",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from datasets import  load_from_disk\n",
    "dataset = load_from_disk(\"demo/cellanno_dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:41:31.133128800Z",
     "start_time": "2024-04-03T08:41:30.781645900Z"
    }
   },
   "id": "a74a434d4e61f544",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column  not in the dataset. Current columns in the dataset: ['cell_type', 'input_ids', 'length', 'organ_major']\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/py39_torch/lib/python3.9/site-packages/datasets/arrow_dataset.py:2810\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   2808\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[1;32m   2809\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[0;32m-> 2810\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/py39_torch/lib/python3.9/site-packages/datasets/arrow_dataset.py:2794\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[0;34m(self, key, **kwargs)\u001B[0m\n\u001B[1;32m   2792\u001B[0m format_kwargs \u001B[38;5;241m=\u001B[39m format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m   2793\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[0;32m-> 2794\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m \u001B[43mquery_table\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2795\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m format_table(\n\u001B[1;32m   2796\u001B[0m     pa_subtable, key, formatter\u001B[38;5;241m=\u001B[39mformatter, format_columns\u001B[38;5;241m=\u001B[39mformat_columns, output_all_columns\u001B[38;5;241m=\u001B[39moutput_all_columns\n\u001B[1;32m   2797\u001B[0m )\n\u001B[1;32m   2798\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[0;32m~/anaconda3/envs/py39_torch/lib/python3.9/site-packages/datasets/formatting/formatting.py:580\u001B[0m, in \u001B[0;36mquery_table\u001B[0;34m(table, key, indices)\u001B[0m\n\u001B[1;32m    578\u001B[0m     _raise_bad_key_type(key)\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 580\u001B[0m     \u001B[43m_check_valid_column_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    581\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    582\u001B[0m     size \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mnum_rows \u001B[38;5;28;01mif\u001B[39;00m indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m table\u001B[38;5;241m.\u001B[39mnum_rows\n",
      "File \u001B[0;32m~/anaconda3/envs/py39_torch/lib/python3.9/site-packages/datasets/formatting/formatting.py:520\u001B[0m, in \u001B[0;36m_check_valid_column_key\u001B[0;34m(key, columns)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_valid_column_key\u001B[39m(key: \u001B[38;5;28mstr\u001B[39m, columns: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[0;32m--> 520\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. Current columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Column  not in the dataset. Current columns in the dataset: ['cell_type', 'input_ids', 'length', 'organ_major']\""
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:43:28.258806800Z",
     "start_time": "2024-04-03T08:43:28.200343500Z"
    }
   },
   "id": "dfc2c9d498e431df",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "classifier = anno_task.classifier\n",
    "dataset_train, _ = anno_task.prepare_data()\n",
    "\n",
    "dataset_train = TensorDataset(torch.tensor(np.array(dataset_train[\"x\"])),\n",
    "                              torch.tensor(dataset_train[\"targets\"]))\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.1)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "print_interval = 2\n",
    "num_epochs = 10\n",
    "\n",
    "# Create a DataLoader to handle batching of the data\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3210a2de1286a9ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "# Create a DataLoader to handle batching of the data\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Move the batch data to the appropriate device (e.g., GPU)\n",
    "        inputs = inputs.to(anno_task.device)\n",
    "        targets = targets.to(anno_task.device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = classifier(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss for monitoring the training progress\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}/{len(train_dataloader)}, Loss: {running_loss}\")\n",
    "        running_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a75564ecab6c392f",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
