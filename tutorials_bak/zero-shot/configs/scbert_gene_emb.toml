# the param for the llm model, to init the foundation model.
model_used = "scbert"
emb_type = 'gene' # gene/cell/gene-expression
model_file = '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/scBERT/ckpt/panglao_pretrain.pth'
vocab_file = '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/data/train_data/gene2vec/gene_vocab.json'
# test h5ad, sc express matrix
model_type = 'scbert'  # scgpt/scmamba/scbert
output_dir = '../../case/result/zero-shot/'
device = 'cuda:2'
bin_num = 7
embsize = 200
distributed = false
batch_size = 8
# the input data info, for the preocess step
max_seq_len = 16907
use_g2v = true
g2v_file = '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/data/train_data/gene2vec/gene2vec_16906.npy'

weight_bias_track = false
project_name = 'biollm'
exp_name = 'scbert_gene_emb_gpu'