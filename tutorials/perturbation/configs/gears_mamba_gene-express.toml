model_used = 'scmamba' # scgpt/scbert/scmamba
emb_type = 'gene-expression' # gene/cell/gene-expression
# mamba args
model_file = '/home/share/huadjyin/home/s_jiangwenjian/proj/scLLM/scGPT/saves/Pretraining/cellxgene/mamba/gst_ori_initemb/best_model.pt'
model_param_file = '/home/share/huadjyin/home/s_jiangwenjian/proj/scLLM/scGPT/saves/Pretraining/cellxgene/mamba/gst_ori_initemb/args.json'
vocab_file = '/home/share/huadjyin/home/s_jiangwenjian/proj/scLLM/scGPT/saves/Pretraining/cellxgene/mamba/gst_ori_initemb/vocab.json'
distributed = false
do_pretrain = false
include_zero_gene = true
task='Embedding'
pad_token = '<pad>'
pad_value = -2
mask_ratio = 0
# the input data info, for the preocess step
hvg_number = 0
data_is_raw = false
input_style='binned'

# gears args
data_dir = '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/bio_model/biollm/case/data/perturbation/norman/mamba'
data_name = 'norman' #'adamson'
split = 'simulation'
seed=1
device = 'cuda'
train_gene_set_size=0.75
batch_size=6
test_batch_size=6
pretrained_emb_size = 512 # same with pretrain
hidden_size = 64 # same with pretrain
epochs = 15
lr = 1e-3
result_dir = '/home/share/huadjyin/home/s_qiuping1/workspace/omics_model/bio_model/biollm/case/data/perturbation/norman/mamba'
use_pretrained = true
pretrain_freeze = true
finetune = true

# wandb setting
weight_bias_track = true
proj_name = 'gears'
exp_name = 'norman_mamba_e15_emb64'
